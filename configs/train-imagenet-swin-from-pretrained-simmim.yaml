# Base config file which stores the default parameters which apply to all model configurations
---

# Whether to run in development/debugging mode; this only uses a few samples in in the train/val dataset
# to quickly run the code as well as sets num_workers=0
dev_mode: False

amp:
  # using amp is determined by device but one can override it here; True = disable amp
  disable_amp: False

  amp_dtype: "float16" # "float16" or "bfloat16"


# Base directory for output files
output_dir: "output"

# Experiment name; this will be used for the output directory; use "development" when developing
exp_name: "simmim-swin-finetune"

# Name of dataset; must be one of the names in the dataset_map dict
dataset_name: "ImageNet"

# Log the train progress every n steps
log_train_steps: 60

# Parameters for the dataset class
dataset:
 
  # Path to the root of the dataset; detects which path to use based on device
  root: "/mnt/d/datasets/imagenet" # linux/windows path
  root_mac: "/Users/bsele/datasets/imagenet-2012" # mac path

  # The image size to resize and crop images to; e.g., 224 = 224x224
  image_size: 224

  # Number of CPU processes the next sample in the dataset; use 0 to only use the main process
  num_workers: 4

train:
  max_norm: 1.0
  
  # batches & epochs
  batch_size: 64
  effective_batch_size: 1024  # batch_size * gradient_accumulation_steps
  validation_batch_size: 32
  epochs: 300

  # Number of epochs to checkpoint after; use 'null' to turn off checkpointing
  ckpt_epochs: 10


  # pretrained model path to then finetune from; typically from simmim pretraining
  # only the model's parameters state_dict are loaded, not the optimizer or scheduler state_dicts
  pretrained_path: /mnt/d/model-registry/backbones/swin_simmim/swin-b-100-epochs-win-6-img-192/train/checkpoints/checkpoint0100.pt

  # Path of weights file (.pt) to resume training; not to be used for loading pretrained weights like simmim
  # use `null` to train a new model from scratch
  checkpoint_path:

# solver params: optimzer, lr scheduler, etc.
solver:

  optimizer:
    # The optimizer to use; must be one of the names in the optimizer_map dict
    name: "adamw"

    params:
      lr: 0.0005 # NOTE: may have to scale lr like the do in code but unsure; i think since I handle gradietn accumulation different I don't
      weight_decay: 0.05

  # TODO: figure out which scheduler and optimizer swin uses
  lr_scheduler:
    name: "warmup_cosine_decay"
    step_lr_on: "steps" # step the lr after n "epochs" or "steps"

    params:
      num_cycles: 1 # 1 => only one decay, no restarts
      warmup_epochs: 20 # TODO might need to adjust based on num epochs I choose
      warmup_min_lr: 5.e-6


# GPU parameters
cuda:  
  # List of GPU devices to use
  gpus: [1]


# Reproducibility information
reproducibility:
  seed: 42

