# Base config file which stores the default parameters which apply to all model configurations
---

# Whether to run in development/debugging mode; this only uses a few samples in in the train/val dataset
# to quickly run the code as well as sets num_workers=0
dev_mode: False

# using amp is determined by device but one can override it here; True = disable amp
disable_amp: False

# Base directory for output files; do not change this 
output_dir: "output/train"

# Experiment name; this will be used for the output directory; use "development" when developing
exp_name: "development"

# Name of dataset; must be one of the names in the dataset_map dict
dataset_name: "ImageNet"

# Log the train progress every n steps
log_train_steps: 60

# Parameters for the dataset class
dataset:
 
  # Path to the root of the dataset; detects which path to use based on device
  root: "/mnt/d/datasets/imagenet" # linux/windows path
  root_mac: "/Users/bsele/datasets/imagenet-2012" # mac path

  # The image size to resize and crop images to; e.g., 224 = 224x224
  image_size: 224

  # Number of CPU processes the next sample in the dataset; use 0 to only use the main process
  num_workers: 4

train:
  max_norm: 1.0
  
  # batches & epochs
  batch_size: 32
  effective_batch_size: 1024  # batch_size * gradient_accumulation_steps
  validation_batch_size: 32
  epochs: 300

  # Number of epochs to checkpoint after; use 'null' to turn off checkpointing
  ckpt_epochs: 15

  # Path of weights file (.pt) to resume training; 
  # use `null` to train a new model from scratch
  checkpoint_path: #"/home/bselee/programming/classifiers/output/train/development/2025_04_25-07_53_49_PM/checkpoints/best_acc1_62-95.pt"

# solver params: optimzer, lr scheduler, etc.
solver:

  optimizer:
    # The optimizer to use; must be one of the names in the optimizer_map dict
    name: "adamw"

    params:
      lr: 0.001
      weight_decay: 0.05

  # TODO: figure out which scheduler and optimizer swin uses
  lr_scheduler:
    name: "reduce_lr_on_plateau"
    step_lr_on: "epochs" # step the lr after n "epochs" or "steps"
    
    params:
      mode: "min"


# GPU parameters
cuda:
  # List of GPU devices to use
  gpus: [0]


# Reproducibility information
reproducibility:
  seed: 42

